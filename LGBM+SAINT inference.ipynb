{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import riiideducation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下是一些生成特征的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为在比赛中test_data 以api方式生成，每一批传过来数据，包含当前批次的数据的特征，和上一批的数据的label。\n",
    "# 所以当你拿到test_data需要生成特征时，你需要用你记录的这个用户以前答题情况来生成特征（第1个函数的作用）\n",
    "# 但是你需要在下一批test_data传过来的时候才能拿到label，你才能（用第2个函数）更新你的对这个用户的记录。\n",
    "def add_user_feats_without_update(df):\n",
    "    ucc = np.zeros(len(df), dtype=np.uint16)\n",
    "    uac = np.zeros(len(df), dtype=np.uint16)\n",
    "    uqcor = np.zeros(len(df), dtype=np.uint8)\n",
    "    uqcnt = np.zeros(len(df), dtype=np.uint8)\n",
    "    utdiff = np.zeros(len(df), dtype=np.uint64)\n",
    "    utdiff_mean = np.zeros(len(df), dtype=np.uint64) \n",
    "    uelapdiff = np.zeros(len(df), dtype=np.float32)  \n",
    "    uq_timediff = np.zeros(len(df), dtype=np.uint64) \n",
    "    for cnt,row in enumerate(df[['user_id', 'content_id','timestamp','prior_question_elapsed_time']].itertuples(index=False)): \n",
    "        if row[0] in curr_u_dict:\n",
    "            ucc[cnt] = curr_u_dict[row[0]][\"ucc\"]\n",
    "            uac[cnt] = curr_u_dict[row[0]][\"uac\"]\n",
    "            utdiff[cnt] = row[2] - curr_u_dict[row[0]][\"uts\"]\n",
    "            utdiff_mean[cnt] = curr_u_dict[row[0]][\"utsdiff\"][1] / curr_u_dict[row[0]][\"utsdiff\"][0] \n",
    "            uelapdiff[cnt] = row[3] - curr_u_dict[row[0]][\"uelapdiff\"] \n",
    "            if row[1] in curr_u_dict[row[0]]:\n",
    "                tmp_idx = curr_u_dict[row[0]][row[1]]\n",
    "                uq_timediff[cnt] =  row[2] - np_uctdiff_cnt[tmp_idx] \n",
    "                uqcor[cnt] = np_cor_cnt[tmp_idx]\n",
    "                uqcnt[cnt] = np_all_cnt[tmp_idx]\n",
    "            else:\n",
    "                uqcor[cnt] = 0; uqcnt[cnt] = 0\n",
    "                uq_timediff[cnt] = 0 \n",
    "        else:\n",
    "            ucc[cnt] = 0; uac[cnt] = 0\n",
    "            uqcor[cnt] = 0; uqcnt[cnt] = 0\n",
    "            utdiff[cnt] = 0; utdiff_mean[cnt] = 0; \n",
    "            uelapdiff[cnt] = 0; uq_timediff[cnt] = 0 \n",
    "            \n",
    "    user_feats_df = pd.DataFrame({'curr_user_correct_cnt':ucc, # 用户当前答题正确的次数\n",
    "                                  'curr_user_answer_cnt':uac, # 用户当前答题总次数\n",
    "                                  'curr_uq_correct_cnt':uqcor, # 用户回答某一个问题正确的次数\n",
    "                                  'curr_uq_answer_cnt':uqcnt, # 用户回答某一个问题的总次数\n",
    "                                  'curr_user_time_diff':utdiff, # 用户当前距离他第一次答题，过去的时间\n",
    "                                  'curr_user_time_diff_mean':utdiff_mean,  # 用户每一次答题的平均间隔\n",
    "                                  'curr_user_elapsed_time_diff':uelapdiff, # 用户回答上一组问题的平均时间\n",
    "                                  'curr_uq_time_diff':uq_timediff # 用户答题时，距离上次回答这个相同问题过去多少时间\n",
    "                                 }) \n",
    "    user_feats_df['curr_uq_acc'] = user_feats_df['curr_uq_correct_cnt'] / user_feats_df['curr_uq_answer_cnt']\n",
    "    user_feats_df['curr_uq_acc'].fillna(0.680, inplace=True)\n",
    "    user_feats_df['curr_uq_acc'] = user_feats_df['curr_uq_acc'].astype(np.float32)\n",
    "    user_feats_df['curr_uq_correct_cnt'] = user_feats_df['curr_uq_correct_cnt'].where(user_feats_df['curr_uq_correct_cnt'] <= 4, 4)\n",
    "    user_feats_df['curr_uq_answer_cnt'] = user_feats_df['curr_uq_answer_cnt'].where(user_feats_df['curr_uq_answer_cnt'] <= 4, 4)\n",
    "    user_feats_df['curr_user_acc'] = user_feats_df['curr_user_correct_cnt'] / user_feats_df['curr_user_answer_cnt']\n",
    "    user_feats_df['curr_user_acc'].fillna(0.680, inplace=True)\n",
    "    user_feats_df['curr_user_acc'] = user_feats_df['curr_user_acc'].astype(np.float32)\n",
    "    user_feats_df['curr_user_time_diff_mean'].fillna(0, inplace=True)  \n",
    "    user_feats_df['curr_user_elapsed_time_diff'].fillna(0, inplace=True) \n",
    "    user_feats_df['curr_uq_time_diff'].fillna(0, inplace=True)  \n",
    "    df = pd.concat([df, user_feats_df], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_user_feats(df):\n",
    "    global idx\n",
    "    for row in df[['user_id','content_id','answered_correctly','timestamp', 'content_type_id','prior_question_elapsed_time',]].values: \n",
    "        if row[4] == 0:\n",
    "            if row[0] in curr_u_dict:\n",
    "                curr_u_dict[row[0]][\"ucc\"] += row[2]\n",
    "                curr_u_dict[row[0]][\"uac\"] += 1\n",
    "                curr_u_dict[row[0]][\"uts\"] = row[3]\n",
    "                curr_u_dict[row[0]][\"utsdiff\"][0] += 1 \n",
    "                curr_u_dict[row[0]][\"utsdiff\"][1] += row[3] \n",
    "                curr_u_dict[row[0]][\"uelapdiff\"] = row[5] \n",
    "                if row[1] in curr_u_dict[row[0]]:\n",
    "                    tmp_idx = curr_u_dict[row[0]][row[1]]\n",
    "                    np_uctdiff_cnt[tmp_idx] = row[3] \n",
    "                    np_cor_cnt[tmp_idx] += row[2]\n",
    "                    np_all_cnt[tmp_idx] += 1\n",
    "                else:\n",
    "                    curr_u_dict[row[0]][row[1]] = idx\n",
    "                    np_uctdiff_cnt[idx] = row[3] \n",
    "                    np_cor_cnt[idx] += row[2]\n",
    "                    np_all_cnt[idx] += 1\n",
    "                    idx += 1\n",
    "            else:\n",
    "                curr_u_dict[row[0]] = {}\n",
    "                curr_u_dict[row[0]][\"ucc\"] = row[2]\n",
    "                curr_u_dict[row[0]][\"uac\"] = 1\n",
    "                curr_u_dict[row[0]][\"uts\"] = row[3]\n",
    "                curr_u_dict[row[0]][\"utsdiff\"] = [1, row[3]] \n",
    "                curr_u_dict[row[0]][\"uelapdiff\"] = row[5] \n",
    "                curr_u_dict[row[0]][row[1]] = idx\n",
    "                np_uctdiff_cnt[idx] = row[3] \n",
    "                np_cor_cnt[idx] += row[2]\n",
    "                np_all_cnt[idx] += 1\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/lectures.csv')\n",
    "lectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\n",
    "lectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])\n",
    "lectures_df['content_type_id'] = 1\n",
    "\n",
    "q_taglist_df = pd.read_csv(\"/kaggle/input/riiid-test-answer-prediction/questions.csv\")[['tags']].astype(str)\n",
    "q_taglist_df[\"tags_l\"] = [x.split() for x in q_taglist_df.tags.values]\n",
    "q_taglist_df['content_type_id'] = 0\n",
    "q_taglist_df.drop(\"tags\", axis=1, inplace=True)\n",
    "q_taglist_df.drop(10033, axis=0, inplace=True) # nan\n",
    "\n",
    "def add_lectures_feats(df, curr_dict):\n",
    "    new_df = df[[\"row_id\", \"user_id\", \"timestamp\", \"content_id\", \"content_type_id\"]]\n",
    "    new_df = new_df.merge(lectures_df, how=\"left\", left_on = [\"content_id\",\"content_type_id\"], right_on = [\"lecture_id\",\"content_type_id\"])\n",
    "    new_df = new_df.merge(q_taglist_df, how=\"left\", left_on = [\"content_id\",\"content_type_id\"], right_on = [q_taglist_df.index,\"content_type_id\"])\n",
    "    new_df = new_df.sort_values([\"timestamp\"])\n",
    "    new_df = new_df[['timestamp', 'user_id', 'content_type_id','tag','part_1','part_2','part_3','part_4','part_5','part_6','part_7',\n",
    "                     'type_of_concept','type_of_intention','type_of_solving_question','type_of_starter','tags_l','row_id']]\n",
    "    ulc_lb = np.zeros(len(df), dtype=\"int8\")\n",
    "    part1_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part2_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part3_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part4_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part5_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part6_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part7_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    type_of_concept_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    type_of_intention_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    type_of_solving_question_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    type_of_starter_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    has_tags_l = np.zeros(len(df), dtype=\"float32\")\n",
    "    \n",
    "    # 0.'timestamp', 1.'user_id', 2.'content_type_id',3.'tag',4.'part_1',5.'part_2',6.'part_3',7.'part_4',8.'part_5',9.'part_6',10.'part_7',\n",
    "    # 11.'type_of_concept',12.'type_of_intention',13.'type_of_solving_question',14.'type_of_starter',15.'tags_l', 16.'row_id'\n",
    "    for cnt,row in enumerate(new_df.itertuples(index=False)):\n",
    "        if row[1] in curr_dict:\n",
    "            if row[2] == 1:\n",
    "                curr_dict[row[1]][\"lecture_bool\"] = 1\n",
    "                curr_dict[row[1]][\"part_1_cnt\"] += int(row[4])\n",
    "                curr_dict[row[1]][\"part_2_cnt\"] += int(row[5])\n",
    "                curr_dict[row[1]][\"part_3_cnt\"] += int(row[6])\n",
    "                curr_dict[row[1]][\"part_4_cnt\"] += int(row[7])\n",
    "                curr_dict[row[1]][\"part_5_cnt\"] += int(row[8])\n",
    "                curr_dict[row[1]][\"part_6_cnt\"] += int(row[9])\n",
    "                curr_dict[row[1]][\"part_7_cnt\"] += int(row[10])\n",
    "                curr_dict[row[1]][\"type_of_concept_cnt\"] += int(row[11])\n",
    "                curr_dict[row[1]][\"type_of_intention_cnt\"] += int(row[12])\n",
    "                curr_dict[row[1]][\"type_of_solving_question_cnt\"] += int(row[13])\n",
    "                curr_dict[row[1]][\"type_of_starter_cnt\"] += int(row[14])\n",
    "                curr_dict[row[1]][\"has_tags\"].add(int(row[3]))\n",
    "        else:\n",
    "            curr_dict[row[1]] = {}\n",
    "            if row[2] == 1:\n",
    "                curr_dict[row[1]][\"lecture_bool\"] = 1\n",
    "                curr_dict[row[1]][\"part_1_cnt\"] = int(row[4])\n",
    "                curr_dict[row[1]][\"part_2_cnt\"] = int(row[5])\n",
    "                curr_dict[row[1]][\"part_3_cnt\"] = int(row[6])\n",
    "                curr_dict[row[1]][\"part_4_cnt\"] = int(row[7])\n",
    "                curr_dict[row[1]][\"part_5_cnt\"] = int(row[8])\n",
    "                curr_dict[row[1]][\"part_6_cnt\"] = int(row[9])\n",
    "                curr_dict[row[1]][\"part_7_cnt\"] = int(row[10])\n",
    "                curr_dict[row[1]][\"type_of_concept_cnt\"] = int(row[11])\n",
    "                curr_dict[row[1]][\"type_of_intention_cnt\"] = int(row[12])\n",
    "                curr_dict[row[1]][\"type_of_solving_question_cnt\"] = int(row[13])\n",
    "                curr_dict[row[1]][\"type_of_starter_cnt\"] = int(row[14])\n",
    "                curr_dict[row[1]][\"has_tags\"] = set([int(row[3])])\n",
    "            else:\n",
    "                curr_dict[row[1]][\"lecture_bool\"] = 0\n",
    "                curr_dict[row[1]][\"part_1_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_2_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_3_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_4_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_5_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_6_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_7_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"type_of_concept_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"type_of_intention_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"type_of_solving_question_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"type_of_starter_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"has_tags\"] = set()\n",
    "        \n",
    "        ulc_lb[cnt] = curr_dict[row[1]][\"lecture_bool\"]\n",
    "        part1_l[cnt] = curr_dict[row[1]][\"part_1_cnt\"]\n",
    "        part2_l[cnt] = curr_dict[row[1]][\"part_2_cnt\"]\n",
    "        part3_l[cnt] = curr_dict[row[1]][\"part_3_cnt\"]\n",
    "        part4_l[cnt] = curr_dict[row[1]][\"part_4_cnt\"]\n",
    "        part5_l[cnt] = curr_dict[row[1]][\"part_5_cnt\"]\n",
    "        part6_l[cnt] = curr_dict[row[1]][\"part_6_cnt\"]\n",
    "        part7_l[cnt] = curr_dict[row[1]][\"part_7_cnt\"]\n",
    "        type_of_concept_l[cnt] = curr_dict[row[1]][\"type_of_concept_cnt\"]\n",
    "        type_of_intention_l[cnt] = curr_dict[row[1]][\"type_of_intention_cnt\"]\n",
    "        type_of_solving_question_l[cnt] = curr_dict[row[1]][\"type_of_solving_question_cnt\"]\n",
    "        type_of_starter_l[cnt] = curr_dict[row[1]][\"type_of_starter_cnt\"]\n",
    "        \n",
    "        if type(row[15]) == list:\n",
    "            tags_has = 0\n",
    "            for tag in row[15]:\n",
    "                if int(tag) in curr_dict[row[1]][\"has_tags\"]:\n",
    "                    tags_has += 1\n",
    "            has_tags_l[cnt] = tags_has/len(row[15])\n",
    "\n",
    "    has_tags_lb = (has_tags_l > 0).astype(\"int8\")\n",
    "\n",
    "    lectures_feats_df = pd.DataFrame({\"curr_lecture_bool\":ulc_lb, # 这个用户之前是否听过讲座\n",
    "                                      \"part_1_cnt\":part1_l, # 这个用户听过多少次part类型为1的讲座\n",
    "                                      \"part_2_cnt\":part2_l, # 这个用户听过多少次part类型为2的讲座\n",
    "                                      \"part_3_cnt\":part3_l, # 这个用户听过多少次part类型为3的讲座\n",
    "                                      \"part_4_cnt\":part4_l, # 这个用户听过多少次part类型为4的讲座\n",
    "                                      \"part_5_cnt\":part5_l, # 这个用户听过多少次part类型为5的讲座\n",
    "                                      \"part_6_cnt\":part6_l, # 这个用户听过多少次part类型为6的讲座\n",
    "                                      \"part_7_cnt\":part7_l, # 这个用户听过多少次part类型为7的讲座\n",
    "                                      \"type_of_concept_cnt\":type_of_concept_l, # 这个用户听过多少次type类型为concept的讲座\n",
    "                                      \"type_of_intention_cnt\":type_of_intention_l, # 这个用户听过多少次type类型为intention的讲座\n",
    "                                      \"type_of_solving_question_cnt\":type_of_solving_question_l, # 这个用户听过多少次type类型为solving_question的讲座\n",
    "                                      \"type_of_starter_cnt\":type_of_starter_l, # 这个用户听过多少次type类型为starter的讲座\n",
    "                                      \"watched_tags_rate\":has_tags_l, # 这个用户在做的这个question所包含的tags，有多少比例是他以前看过相同tags的讲座\n",
    "                                      \"watched_tags_bool\":has_tags_lb,# 这个用户在做的这个question所包含的tags，是否至少有一个tag是他以前看过相同tag的讲座\n",
    "                                     }).set_index(new_df[\"row_id\"])\n",
    "\n",
    "    df = df.merge(lectures_feats_df,how=\"left\",left_on=\"row_id\",right_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def to_letures_dict(df):\n",
    "    da = {}\n",
    "    for t in tqdm(df.itertuples(name=None), total=df.shape[0]):\n",
    "        key = t[0]\n",
    "        sub_dict = {}\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            if col == \"has_tags\":\n",
    "                sub_dict[col] = eval(t[i])\n",
    "            else:\n",
    "                sub_dict[col] = t[i]\n",
    "        da[key] = sub_dict\n",
    "    return da\n",
    "\n",
    "curr_lectures_dict = pd.read_csv(\"../input/merge-data1615/curr_lectures_dict_1615.csv.data\", index_col=0)\n",
    "curr_lectures_dict = to_letures_dict(curr_lectures_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 用户在做某个part类型的题目前，看过多少个part类型的讲座\n",
    "def same_part(df):\n",
    "    same_part_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    for idx,row in enumerate(df.itertuples()):\n",
    "        part_cnt = eval(f\"row.part_{str(int(row.part))}_cnt\")\n",
    "        same_part_l[idx] = part_cnt\n",
    "    df[\"same_part_cnt\"] = same_part_l\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于user-part对（groupby(['user_id', 'part'])）的正确率\n",
    "# 分成两个函数的道理，同上，是因为在比赛中test_data的api生成方式\n",
    "def add_up_without_update(df):\n",
    "    np_up_cnt = np.zeros((len(test_df),2), dtype=np.uint16)\n",
    "    for cnt,row in enumerate(df[['user_id', 'part']].itertuples(index=False)): \n",
    "        if (row[0],row[1]) in part_user_d:\n",
    "            np_up_cnt[cnt] = [part_user_d[(row[0],row[1])][\"count\"], part_user_d[(row[0],row[1])][\"sum\"]] \n",
    "    curr_user_part_df = pd.DataFrame(np_up_cnt,columns=[\"curr_user_part_count\", \"curr_user_part_sum\"],index=df.row_id)\n",
    "    curr_user_part_df[\"curr_user_part_acc\"] = (curr_user_part_df[\"curr_user_part_sum\"] / curr_user_part_df[\"curr_user_part_count\"]).fillna(0.68).astype(np.float32)\n",
    "    df = df.merge(curr_user_part_df,how=\"left\", left_on=\"row_id\", right_index=True)\n",
    "    return df\n",
    "\n",
    "def update_up(df):\n",
    "    for row in df[['user_id','part','answered_correctly']].values: \n",
    "        if (row[0], row[1]) in part_user_d:\n",
    "            part_user_d[(row[0], row[1])][\"count\"] += 1\n",
    "            part_user_d[(row[0], row[1])][\"sum\"] += row[2]\n",
    "        else:\n",
    "            part_user_d[(row[0], row[1])] = {'count': 1, 'sum': 1} if row[2] == 1 else {'count': 1, 'sum': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 用户做这题时，距离上次（前一次）做题过去了多少时间\n",
    "# 生成 用户做这题时，距离上上次（前两次）做题过去了多少时间\n",
    "# 生成 用户做这题时，距离上上上次（前三次）做题过去了多少时间\n",
    "def lagtime_for_test(df):\n",
    "    lagtime_mean = 0\n",
    "    lagtime_mean2 = 0\n",
    "    lagtime_mean3 = 0\n",
    "    lagtime = np.zeros(len(df), dtype=np.float32)\n",
    "    lagtime2 = np.zeros(len(df), dtype=np.float32)\n",
    "    lagtime3 = np.zeros(len(df), dtype=np.float32)\n",
    "    for i, (user_id,\n",
    "            content_type_id,\n",
    "            timestamp,\n",
    "            content_id,) in enumerate(zip(df['user_id'].values, df['content_type_id'].values, df['timestamp'].values, df['content_id'].values)):\n",
    "        if content_type_id==0:\n",
    "            if user_id in max_timestamp_u_dict['max_time_stamp'].keys():\n",
    "                lagtime[i]=timestamp-max_timestamp_u_dict['max_time_stamp'][user_id]\n",
    "                if(max_timestamp_u_dict2['max_time_stamp2'][user_id]==lagtime_mean2):\n",
    "                    lagtime2[i]=lagtime_mean2\n",
    "                    lagtime3[i]=lagtime_mean3\n",
    "                else:\n",
    "                    lagtime2[i]=timestamp-max_timestamp_u_dict2['max_time_stamp2'][user_id]\n",
    "                    if(max_timestamp_u_dict3['max_time_stamp3'][user_id]==lagtime_mean3):\n",
    "                        lagtime3[i]=lagtime_mean3\n",
    "                    else:\n",
    "                        lagtime3[i]=timestamp-max_timestamp_u_dict3['max_time_stamp3'][user_id]\n",
    "                    max_timestamp_u_dict3['max_time_stamp3'][user_id]=max_timestamp_u_dict2['max_time_stamp2'][user_id]\n",
    "                max_timestamp_u_dict2['max_time_stamp2'][user_id]=max_timestamp_u_dict['max_time_stamp'][user_id]\n",
    "                max_timestamp_u_dict['max_time_stamp'][user_id]=timestamp\n",
    "            else:\n",
    "                lagtime[i]=lagtime_mean\n",
    "                max_timestamp_u_dict['max_time_stamp'].update({user_id:timestamp})\n",
    "                lagtime2[i]=lagtime_mean2\n",
    "                max_timestamp_u_dict2['max_time_stamp2'].update({user_id:lagtime_mean2})\n",
    "                lagtime3[i]=lagtime_mean3\n",
    "                max_timestamp_u_dict3['max_time_stamp3'].update({user_id:lagtime_mean3})\n",
    "    df[\"lagtime\"]= lagtime\n",
    "    df[\"lagtime2\"]= lagtime2\n",
    "    df[\"lagtime3\"]= lagtime3\n",
    "    df[\"lagtime\"].fillna(lagtime_mean, inplace=True)\n",
    "    df[\"lagtime2\"].fillna(lagtime_mean2, inplace=True)\n",
    "    df[\"lagtime3\"].fillna(lagtime_mean3, inplace=True)\n",
    "    df[\"lagtime\"] = df[\"lagtime\"].astype(\"uint64\")\n",
    "    df[\"lagtime2\"] = df[\"lagtime2\"].astype(\"uint64\")\n",
    "    df[\"lagtime3\"] = df[\"lagtime3\"].astype(\"uint64\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入特征和模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在gen_data中dump的那些特征，现在全部load进来，\n",
    "content_answers_df = pickle.load(open(\"../input/merge-data1615/content_answers_df_1615.pkl.data\",\"rb\"))\n",
    "ques_df = pickle.load(open(\"../input/merge-data1615/ques_df_1615.pkl.data\",\"rb\"))\n",
    "task_user_df = pickle.load(open(\"../input/merge-data1615/task_user_df_1615.pkl.data\",\"rb\"))\n",
    "qdf = pickle.load(open(\"../input/merge-data1615/qdf_1615.pkl.data\",\"rb\"))\n",
    "question_elapsed_time_df = pickle.load(open(\"../input/merge-data1615/question_elapsed_time_mean.pkl.data\",\"rb\")) \n",
    "content_explation_agg = pickle.load(open(\"../input/merge-data1615/content_explation_agg_2015.pkl.data\",\"rb\")) \n",
    "max_timestamp_u_dict = pickle.load(open(\"../input/merge-data1615/max_timestamp_u_dict_2015.pkl.data\",\"rb\")) \n",
    "max_timestamp_u_dict2 = pickle.load(open(\"../input/merge-data1615/max_timestamp_u_dict2_2015.pkl.data\",\"rb\")) \n",
    "max_timestamp_u_dict3 = pickle.load(open(\"../input/merge-data1615/max_timestamp_u_dict3_2015.pkl.data\",\"rb\")) \n",
    "\n",
    "curr_u_dict = pickle.load(open(\"../input/merge-data1615/curr_u_dict_1615.pkl.data\",\"rb\"))\n",
    "np_cor_cnt = pickle.load(open(\"../input/merge-data1615/np_cor_cnt_1615.pkl.data\",\"rb\"))\n",
    "np_all_cnt = pickle.load(open(\"../input/merge-data1615/np_all_cnt_1615.pkl.data\",\"rb\"))\n",
    "np_uctdiff_cnt = pickle.load(open(\"../input/merge-data1615/np_uctdiff_cnt_1615.pkl.data\",\"rb\"))\n",
    "    \n",
    "ques_df3 = pickle.load(open(\"../input/merge-data1615/ques_df3_2015.pkl.data\",\"rb\")) \n",
    "content_elapsed_time_agg = pickle.load(open(\"../input/merge-data1615/content_elapsed_time_agg_2015.pkl.data\",\"rb\")) \n",
    "content_had_explanation_agg = pickle.load(open(\"../input/merge-data1615/content_had_explanation_agg_2015.pkl.data\",\"rb\")) \n",
    "    \n",
    "curr_up_dict_df = pickle.load(open(\"../input/merge-data1615/curr_up_dict_df_1615.pkl.data\",\"rb\"))\n",
    "part_user_d = curr_up_dict_df.to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 86867031 #记curr_u_dict目前的序号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = joblib.load('../input/lgb-2113model/lgb_2113.model') # 导入train出来的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有要用的特征\n",
    "features = [\n",
    "    \"part_bundle_id\",\n",
    "    \n",
    "    'content_elapsed_time',\n",
    "    'content_had_explanation',\n",
    "    \n",
    "    'lagtime2',\n",
    "    'lagtime3',\n",
    "    \n",
    "    'content_explation_false_mean',\n",
    "    'content_explation_true_mean',\n",
    "    \n",
    "    'curr_user_part_acc', \n",
    "    'curr_user_part_count', \n",
    "    'curr_user_part_sum',  \n",
    "    'curr_uq_time_diff', \n",
    "    'curr_user_time_diff',\n",
    "    'curr_user_time_diff_mean',\n",
    "    'curr_user_elapsed_time_diff',\n",
    "\n",
    "    'avg_task_seen_cumsum',\n",
    "    'content_mean_acc',\n",
    "    'content_cnt',\n",
    "    'corr_question_elapsed_time_mean', \n",
    "    'incorr_question_elapsed_time_mean',\n",
    "    \n",
    "    \"watched_tags_rate\",\n",
    "    \"watched_tags_bool\",\n",
    "    'tags_acc',\n",
    "    'part',\n",
    "    'part_bundle_acc', \n",
    "    \n",
    "    'part_1_cnt', 'part_2_cnt', 'part_3_cnt', 'part_4_cnt', 'part_5_cnt', 'part_6_cnt', 'part_7_cnt', \n",
    "    'type_of_concept_cnt', 'type_of_intention_cnt', 'type_of_solving_question_cnt', 'type_of_starter_cnt', \n",
    "    \"same_part_cnt\",\n",
    "    \n",
    "    'curr_lecture_bool',\n",
    "    'curr_user_correct_cnt', \n",
    "    'curr_user_answer_cnt',\n",
    "    'curr_user_acc',\n",
    "    'hmean_acc',\n",
    "    'curr_uq_correct_cnt',\n",
    "    'curr_uq_answer_cnt',\n",
    "    'curr_uq_acc',\n",
    "    'prior_question_elapsed_time',\n",
    "    'prior_question_had_explanation', \n",
    "]\n",
    "\n",
    "target = 'answered_correctly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 100\n",
    "n_part = 7\n",
    "D_MODEL = 256\n",
    "N_LAYER = 2\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_time_lag(df, time_dict):\n",
    "    tt = np.zeros(len(df), dtype=np.int64)\n",
    "    for ind, row in enumerate(df[['user_id','timestamp','task_container_id']].values):\n",
    "        if row[0] in time_dict.keys():\n",
    "            if row[2]-time_dict[row[0]][1] == 0:\n",
    "                tt[ind] = time_dict[row[0]][2]\n",
    "            else:\n",
    "                t_last = time_dict[row[0]][0]\n",
    "                task_ind_last = time_dict[row[0]][1]\n",
    "                tt[ind] = row[1]-t_last\n",
    "                time_dict[row[0]] = (row[1], row[2], tt[ind])\n",
    "        else:\n",
    "            # time_dict : timestamp, task_container_id, lag_time\n",
    "            time_dict[row[0]] = (row[1], row[2], -1)\n",
    "            tt[ind] =  0\n",
    "    df[\"time_lag\"] = tt\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, state_size=200):\n",
    "        super(FFN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.lr1 = nn.Linear(state_size, state_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(state_size, state_size)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lr2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def future_mask(seq_length):\n",
    "    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "    return torch.from_numpy(future_mask)\n",
    "\n",
    "\n",
    "class SAINTModel(nn.Module):\n",
    "    def __init__(self, n_skill, n_part, max_seq=MAX_SEQ, embed_dim= 128, elapsed_time_cat_flag = True):\n",
    "        super(SAINTModel, self).__init__()\n",
    "\n",
    "        self.n_skill = n_skill\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_cat = n_part\n",
    "        self.elapsed_time_cat_flag = elapsed_time_cat_flag\n",
    "\n",
    "        self.e_embedding = nn.Embedding(self.n_skill+1, embed_dim) ## exercise\n",
    "        self.c_embedding = nn.Embedding(self.n_cat+1, embed_dim) ## category\n",
    "        self.pos_embedding = nn.Embedding(max_seq-1, embed_dim) ## position\n",
    "        self.res_embedding = nn.Embedding(2+1, embed_dim) ## response\n",
    "\n",
    "\n",
    "        if self.elapsed_time_cat_flag == True:\n",
    "            self.elapsed_time_embedding = nn.Embedding(300+1, embed_dim) ## elapsed time (the maximum elasped time is 300)\n",
    "            self.lag_embedding1 = nn.Embedding(300+1, embed_dim) ## lag time1 for 300 seconds\n",
    "            self.lag_embedding2 = nn.Embedding(1440+1, embed_dim) ## lag time2 for 1440 minutes\n",
    "            self.lag_embedding3 = nn.Embedding(365+1, embed_dim) ## lag time3 for 365 days\n",
    "\n",
    "        else:\n",
    "            self.elapsed_time_embedding = nn.Linear(1, embed_dim, bias=False) ## elapsed time\n",
    "            self.lag_embedding = nn.Linear(1, embed_dim, bias=False) ## lag time\n",
    "\n",
    "\n",
    "        self.exp_embedding = nn.Embedding(2+1, embed_dim) ## user had explain\n",
    "\n",
    "        self.transformer = nn.Transformer(nhead=8, d_model = embed_dim, num_encoder_layers= N_LAYER, num_decoder_layers= N_LAYER, dropout = DROPOUT)\n",
    "\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.layer_normal = nn.LayerNorm(embed_dim) \n",
    "        self.ffn = FFN(embed_dim)\n",
    "        self.pred = nn.Linear(embed_dim, 1)\n",
    "    \n",
    "    def forward(self, question, part, response, elapsed_time, lag_time, exp):\n",
    "\n",
    "        device = question.device  \n",
    "\n",
    "        ## embedding layer\n",
    "        question = self.e_embedding(question)\n",
    "        part = self.c_embedding(part)\n",
    "        pos_id = torch.arange(question.size(1)).unsqueeze(0).to(device)\n",
    "        pos_id = self.pos_embedding(pos_id)\n",
    "        res = self.res_embedding(response)\n",
    "        exp = self.exp_embedding(exp)\n",
    "\n",
    "        if self.elapsed_time_cat_flag == True:\n",
    "\n",
    "            ## feature engineering\n",
    "            ## elasped time\n",
    "            elapsed_time = torch.true_divide(elapsed_time, 1000)\n",
    "            elapsed_time = torch.round(elapsed_time)\n",
    "            elapsed_time = torch.where(elapsed_time.float() <= 300, elapsed_time, torch.tensor(300.0).to(device)).long()\n",
    "            elapsed_time = self.elapsed_time_embedding(elapsed_time)\n",
    "\n",
    "            ## lag_time1\n",
    "            lag_time = torch.true_divide(lag_time, 1000)\n",
    "            lag_time = torch.round(lag_time)\n",
    "            lag_time1 = torch.where(lag_time.float() <= 300, lag_time, torch.tensor(300.0).to(device)).long()\n",
    "\n",
    "            ## lag_time2\n",
    "            lag_time = torch.true_divide(lag_time, 60)\n",
    "            lag_time = torch.round(lag_time)\n",
    "            lag_time2 = torch.where(lag_time.float() <= 1440, lag_time, torch.tensor(1440.0).to(device)).long()\n",
    "\n",
    "            ## lag_time3\n",
    "            lag_time = torch.true_divide(lag_time, 1440)\n",
    "            lag_time = torch.round(lag_time)\n",
    "            lag_time3 = torch.where(lag_time.float() <= 365, lag_time, torch.tensor(365.0).to(device)).long()\n",
    "\n",
    "            ## lag time\n",
    "            lag_time1 = self.lag_embedding1(lag_time1) \n",
    "            lag_time2 = self.lag_embedding2(lag_time2) \n",
    "            lag_time3 = self.lag_embedding3(lag_time3)\n",
    "\n",
    "        else:\n",
    "\n",
    "            elapsed_time = elapsed_time.view(-1,1)\n",
    "            elapsed_time = self.elapsed_time_embedding(elapsed_time)\n",
    "            elapsed_time = elapsed_time.view(-1, MAX_SEQ-1, self.embed_dim)\n",
    "\n",
    "            lag_time = lag_time.view(-1,1)\n",
    "            lag_time = self.lag_embedding(lag_time)\n",
    "            lag_time = lag_time.view(-1, MAX_SEQ-1, self.embed_dim)\n",
    "\n",
    "            # elapsed_time = elapsed_time.view(-1, MAX_SEQ-1, 1)  ## [batch, s_len] => [batch, s_len, 1]\n",
    "            # elapsed_time = self.elapsed_time_embedding(elapsed_time)\n",
    "\n",
    "\n",
    "        enc = question + part + pos_id + exp\n",
    "        dec = pos_id + res + elapsed_time + lag_time1 + lag_time2 + lag_time3\n",
    "\n",
    "        enc = enc.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n",
    "        dec = dec.permute(1, 0, 2)\n",
    "        mask = future_mask(enc.size(0)).to(device)\n",
    "\n",
    "        att_output = self.transformer(enc, dec, src_mask=mask, tgt_mask=mask, memory_mask = mask)\n",
    "        att_output = self.layer_normal(att_output)\n",
    "        att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n",
    "\n",
    "        x = self.ffn(att_output)\n",
    "        x = self.layer_normal(x + att_output)\n",
    "        x = self.pred(x)\n",
    "\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_skill = 13523\n",
    "group = joblib.load(\"./group.pkl\")\n",
    "questions_df = pd.read_csv('D:/kaggle/input/riiid-test-answer-prediction/questions.csv')\n",
    "time_dict = joblib.load(\"./time_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "saint_model = SAINTModel(n_skill, n_part, embed_dim= D_MODEL)\n",
    "\n",
    "saint_model.load_state_dict(torch.load(\"./saint_plus_model.pt\")) # 用你自己生成的模型\n",
    "\n",
    "saint_model.to(device)\n",
    "saint_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, samples, test_df, n_skills, max_seq=MAX_SEQ): \n",
    "        super(TestDataset, self).__init__()\n",
    "        self.samples = samples\n",
    "        self.user_ids = [x for x in test_df[\"user_id\"].unique()]\n",
    "        self.test_df = test_df\n",
    "        self.n_skill = n_skills\n",
    "        self.max_seq = max_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.test_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        test_info = self.test_df.iloc[index]\n",
    "\n",
    "        user_id = test_info[\"user_id\"]\n",
    "        target_id = test_info[\"content_id\"]\n",
    "        part = test_info[\"part\"]\n",
    "        pri_quest_elap = test_info[\"prior_question_elapsed_time\"]\n",
    "        time_lag = test_info[\"time_lag\"]\n",
    "        pri_quest_exp = test_info[\"prior_question_had_explanation\"]\n",
    "        \n",
    "        q = np.zeros(self.max_seq, dtype=int)\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        res = np.zeros(self.max_seq, dtype=int)\n",
    "        p = np.zeros(self.max_seq, dtype=int)\n",
    "        pri_elap = np.zeros(self.max_seq, dtype=int)\n",
    "        lag = np.zeros(self.max_seq, dtype=int)\n",
    "        pri_exp = np.zeros(self.max_seq, dtype=int)\n",
    "\n",
    "        if user_id in self.samples.index:\n",
    "            q_, qa_, p_, pri_elap_, lag_, pri_exp_ = self.samples[user_id]\n",
    "            \n",
    "            seq_len = len(q_)\n",
    "            \n",
    "            ## for zero padding\n",
    "            q_ = q_+1\n",
    "            pri_exp_ = pri_exp_ + 1\n",
    "            res_ = qa_ + 1\n",
    "            \n",
    "\n",
    "            if seq_len >= self.max_seq:\n",
    "                q = q_[-self.max_seq:]\n",
    "                qa = qa_[-self.max_seq:]\n",
    "                res = res_[-self.max_seq:]\n",
    "                p = p_[-self.max_seq:]\n",
    "                pri_elap = pri_elap_[-self.max_seq:]\n",
    "                lag = lag_[-self.max_seq:]\n",
    "                pri_exp = pri_exp_[-self.max_seq:]\n",
    "                \n",
    "            else:\n",
    "                q[-seq_len:] = q_\n",
    "                qa[-seq_len:] = qa_\n",
    "                res[-seq_len:] = res_\n",
    "                p[-seq_len:] = p_\n",
    "                pri_elap[-seq_len:] = pri_elap_\n",
    "                lag[-seq_len:] = lag_\n",
    "                pri_exp[-seq_len:] = pri_exp_\n",
    "                \n",
    "        \n",
    "        exercise = np.append(q[2:], [target_id+1])\n",
    "        part = np.append(p[2:], [part])\n",
    "        elap = np.append(pri_elap[2:], [pri_quest_elap])\n",
    "        lag = np.append(lag[2:], [time_lag])\n",
    "        pri_exp = np.append(pri_exp[2:], [pri_quest_exp+1])\n",
    "\n",
    "        response = res[1:]\n",
    "\n",
    "        return  exercise, part, response, elap, lag, pri_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_test_df = None\n",
    "\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    if (previous_test_df is not None) & (psutil.virtual_memory().percent < 90):\n",
    "        previous_test_df['answered_correctly'] = eval(test_df['prior_group_answers_correct'].iloc[0])  # 将上一批的test_df的特征和label合并起来\n",
    "        update_user_feats(previous_test_df) # 更新用户相关的特征的记录\n",
    "        previous_test_df = previous_test_df[previous_test_df.content_type_id == False] # 只保留question行\n",
    "        \n",
    "        update_up(previous_test_df) # 更新user-part对（groupby(['user_id', 'part'])）的记录\n",
    "        \n",
    "        previous_test_df = feature_time_lag(previous_test_df, time_dict)# 生成lag time\n",
    "\n",
    "        prev_group = previous_test_df[['user_id', 'content_id', 'answered_correctly', 'part', 'prior_question_elapsed_time', 'time_lag', 'prior_question_had_explanation']].groupby('user_id').apply(lambda r: (\n",
    "            r['content_id'].values,\n",
    "            r['answered_correctly'].values,\n",
    "            r['part'].values,\n",
    "            r['prior_question_elapsed_time'].values,\n",
    "            r['time_lag'].values,\n",
    "            r['prior_question_had_explanation'].values))\n",
    "        \n",
    "        for prev_user_id in prev_group.index:\n",
    "            if prev_user_id in group.index:\n",
    "                group[prev_user_id] = (\n",
    "                    np.append(group[prev_user_id][0], prev_group[prev_user_id][0])[-MAX_SEQ:], \n",
    "                    np.append(group[prev_user_id][1], prev_group[prev_user_id][1])[-MAX_SEQ:],\n",
    "                    np.append(group[prev_user_id][2], prev_group[prev_user_id][2])[-MAX_SEQ:],\n",
    "                    np.append(group[prev_user_id][3], prev_group[prev_user_id][3])[-MAX_SEQ:],\n",
    "                    np.append(group[prev_user_id][4], prev_group[prev_user_id][4])[-MAX_SEQ:],\n",
    "                    np.append(group[prev_user_id][5], prev_group[prev_user_id][5])[-MAX_SEQ:]\n",
    "                )\n",
    " \n",
    "            else:\n",
    "                group[prev_user_id] = (\n",
    "                    prev_group[prev_user_id][0], \n",
    "                    prev_group[prev_user_id][1],\n",
    "                    prev_group[prev_user_id][2],\n",
    "                    prev_group[prev_user_id][3],\n",
    "                    prev_group[prev_user_id][4],\n",
    "                    prev_group[prev_user_id][5]\n",
    "                )\n",
    "\n",
    "    \n",
    "    test_df.prior_question_elapsed_time = test_df.prior_question_elapsed_time.fillna(0) # 填充空值为0\n",
    "    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype(int) # 填充空值为false\n",
    "    test_df = test_df.merge(questions_df[[\"question_id\",\"part\"]], how = \"left\",left_on = 'content_id', right_on = 'question_id') #加入part特征\n",
    "              \n",
    "    previous_test_df = test_df.copy()  # 复制一份test_df，留到下一批test_df来的时候（有当前批的label）使用。\n",
    "    test_df = add_lectures_feats(test_df, curr_lectures_dict) # 生成一些和lecture相关的特征        \n",
    "    test_df = test_df[test_df.content_type_id == False]  # 只保留question行\n",
    "    \n",
    "    test_df = add_user_feats_without_update(test_df) # 生成一些用户相关的特征\n",
    "    \n",
    "    # 导入的特征，直接merge\n",
    "    test_df = test_df.merge(content_answers_df, how='left', left_on='content_id', right_index=True)\n",
    "    test_df = test_df.merge(ques_df, how='left', left_on='content_id',right_index=True)\n",
    "    test_df = test_df.merge(task_user_df, how='left', left_on='task_container_id',right_index=True)\n",
    "    test_df = test_df.merge(qdf,how=\"left\",left_on=\"content_id\",right_index=True)\n",
    "    test_df = test_df.merge(ques_df3, how=\"left\", left_on=\"content_id\", right_index=True)\n",
    "    test_df = test_df.merge(content_elapsed_time_agg, how=\"left\", left_on=\"content_id\", right_index=True)\n",
    "    test_df = test_df.merge(content_had_explanation_agg, how=\"left\", left_on=\"content_id\", right_index=True)\n",
    "    test_df = test_df.merge(question_elapsed_time_df,on = \"content_id\", how = \"left\")\n",
    "    test_df.corr_question_elapsed_time_mean = test_df.corr_question_elapsed_time_mean.fillna(-1).astype(\"float32\")\n",
    "    test_df.incorr_question_elapsed_time_mean = test_df.incorr_question_elapsed_time_mean.fillna(-1).astype(\"float32\")\n",
    "    test_df = test_df.merge(content_explation_agg,how=\"left\",left_on=\"content_id\",right_on=\"content_id\") \n",
    "    test_df[\"content_explation_false_mean\"].fillna(0,inplace=True) \n",
    "    test_df[\"content_explation_true_mean\"].fillna(0,inplace=True)\n",
    "    \n",
    "    test_df = lagtime_for_test(test_df) # 生成 用户做这题时，距离前1~3次做题过去了多少时间\n",
    "    test_df = add_up_without_update(test_df) # 生成user-part对（groupby(['user_id', 'part'])）的正确率\n",
    "    test_df = same_part(test_df) # 生成 用户在做某个part类型的题目前，看过多少个part类型的讲座\n",
    "    \n",
    "    # 和gen_data时一样，一些简单的空值填充和异常值处理。\n",
    "    test_df['hmean_acc'] = 2*((test_df['curr_user_acc']*test_df['content_mean_acc']) /(test_df['curr_user_acc']+test_df['content_mean_acc']))\n",
    "    test_df[\"content_mean_acc\"] = test_df.content_mean_acc.mask((test_df[\"content_cnt\"] < 3), 0.65)\n",
    "    test_df[\"content_mean_acc\"] = test_df.content_mean_acc.mask((test_df[\"content_mean_acc\"] < 0.2) & (test_df[\"content_cnt\"] < 21), 0.2)\n",
    "    test_df[\"content_mean_acc\"] = test_df.content_mean_acc.mask((test_df[\"content_mean_acc\"] > 0.95) & (test_df[\"content_cnt\"] < 21), 0.95)\n",
    "    test_df[\"curr_user_acc\"] = test_df.curr_user_acc.mask((test_df[\"curr_user_acc\"] < 0.2) & (test_df[\"curr_user_answer_cnt\"] < 21), 0.2)\n",
    "    test_df[\"curr_user_acc\"] = test_df.curr_user_acc.mask((test_df[\"curr_user_acc\"] > 0.95) & (test_df[\"curr_user_answer_cnt\"] < 21), 0.95)\n",
    "    \n",
    "    test_df = feature_time_lag(test_df, time_dict) # 生成lag time\n",
    "    \n",
    "    # saint data pipeline\n",
    "    test_dataset = TestDataset(group, test_df, n_skill)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=51200, shuffle=False)\n",
    "    \n",
    "    saint_outs = []\n",
    "\n",
    "    for item in test_dataloader:\n",
    "        exercise = item[0].to(device).long()\n",
    "        part = item[1].to(device).long()\n",
    "        response = item[2].to(device).long()\n",
    "        elapsed_time = item[3].to(device).long()\n",
    "        lag_time = item[4].to(device).long()\n",
    "        pri_exp = item[5].to(device).long()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = saint_model(exercise, part, response, elapsed_time, lag_time, pri_exp)\n",
    "        saint_outs.extend(torch.sigmoid(output)[:, -1].view(-1).data.cpu().numpy())\n",
    "    \n",
    "    saint_outs = np.array(outs,dtype=np.float64) # saint的output\n",
    "    lgb_outs = lgb_model.predict(test_df[features]) # lgbm的output\n",
    "    \n",
    "    test_df['answered_correctly'] = saint_outs * 0.9 + lgb_outs * 0.1 # 9:1的比例\n",
    "    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
