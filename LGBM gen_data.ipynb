{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (98730332, 9) Valid size: (2500000, 9)\n"
     ]
    }
   ],
   "source": [
    "#data from https://www.kaggle.com/its7171/riiid-cross-validation-files 去kaggle下载\n",
    "columns = ['row_id','user_id','timestamp','content_id', \"content_type_id\", 'task_container_id',\n",
    "           'answered_correctly','prior_question_elapsed_time','prior_question_had_explanation']\n",
    "train_df = pd.read_pickle(\"D:/kaggle/input/riiid-test-answer-prediction/cv_data/cv1_train.pickle\")[columns]\n",
    "valid_df = pd.read_pickle(\"D:/kaggle/input/riiid-test-answer-prediction/cv_data/cv1_valid.pickle\")[columns]\n",
    "print(\"Train size:\", train_df.shape,\"Valid size:\", valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个notebook直接运行会需要大约30多g的内存\n",
    "# 如果内存不够建议运行以下代码，通过修改train_data_size来减少数据量，来适应机器\n",
    "# train_data_size = 50_000_000\n",
    "# train_df = train_df.iloc[:train_data_size,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lectures.csv\n",
    "merge lectures.csv文件和question.csv 生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef28f34da87941d0bd3fe6ff0b17e308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=98730332.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109ecdadf5464529b4fc85698a30355b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2500000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 13min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 生成讲座相关的特征（和时间序列相关）\n",
    "# 具体生成特征解释在函数末尾处\n",
    "\n",
    "lectures_df = pd.read_csv('D:/kaggle/input/riiid-test-answer-prediction/lectures.csv')\n",
    "lectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\n",
    "lectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])\n",
    "lectures_df['content_type_id'] = 1\n",
    "\n",
    "q_taglist_df = pd.read_csv(\"D:/kaggle/input/riiid-test-answer-prediction/questions.csv\")[['tags']].astype(str)\n",
    "q_taglist_df[\"tags_l\"] = [x.split() for x in q_taglist_df.tags.values]\n",
    "q_taglist_df['content_type_id'] = 0\n",
    "q_taglist_df.drop(\"tags\", axis=1, inplace=True)\n",
    "q_taglist_df.drop(10033, axis=0, inplace=True) # 第10033个问题，tags 是 nan\n",
    "\n",
    "def add_lectures_feats(df, curr_dict):\n",
    "    new_df = df[[\"row_id\", \"user_id\", \"timestamp\", \"content_id\", \"content_type_id\"]]\n",
    "    new_df = new_df.merge(lectures_df, how=\"left\", left_on = [\"content_id\",\"content_type_id\"], right_on = [\"lecture_id\",\"content_type_id\"])\n",
    "    new_df = new_df.merge(q_taglist_df, how=\"left\", left_on = [\"content_id\",\"content_type_id\"], right_on = [q_taglist_df.index,\"content_type_id\"])\n",
    "    new_df = new_df.sort_values([\"timestamp\"])\n",
    "    new_df = new_df[['timestamp', 'user_id', 'content_type_id','tag','part_1','part_2','part_3','part_4','part_5','part_6','part_7',\n",
    "                     'type_of_concept','type_of_intention','type_of_solving_question','type_of_starter','tags_l','row_id']]\n",
    "    ulc_lb = np.zeros(len(df), dtype=\"int8\")\n",
    "    part1_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part2_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part3_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part4_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part5_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part6_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    part7_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    type_of_concept_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    type_of_intention_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    type_of_solving_question_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    type_of_starter_l = np.zeros(len(df), dtype=\"uint16\")\n",
    "    has_tags_l = np.zeros(len(df), dtype=\"float32\")\n",
    "    \n",
    "    # 0.'timestamp', 1.'user_id', 2.'content_type_id',3.'tag',4.'part_1',5.'part_2',6.'part_3',7.'part_4',8.'part_5',9.'part_6',10.'part_7',\n",
    "    # 11.'type_of_concept',12.'type_of_intention',13.'type_of_solving_question',14.'type_of_starter',15.'tags_l', 16.'row_id'\n",
    "    for cnt,row in enumerate(tqdm(new_df.itertuples(index=False), total=new_df.shape[0])):\n",
    "        if row[1] in curr_dict:\n",
    "            if row[2] == 1:\n",
    "                curr_dict[row[1]][\"lecture_bool\"] = 1\n",
    "                curr_dict[row[1]][\"part_1_cnt\"] += int(row[4])\n",
    "                curr_dict[row[1]][\"part_2_cnt\"] += int(row[5])\n",
    "                curr_dict[row[1]][\"part_3_cnt\"] += int(row[6])\n",
    "                curr_dict[row[1]][\"part_4_cnt\"] += int(row[7])\n",
    "                curr_dict[row[1]][\"part_5_cnt\"] += int(row[8])\n",
    "                curr_dict[row[1]][\"part_6_cnt\"] += int(row[9])\n",
    "                curr_dict[row[1]][\"part_7_cnt\"] += int(row[10])\n",
    "                curr_dict[row[1]][\"type_of_concept_cnt\"] += int(row[11])\n",
    "                curr_dict[row[1]][\"type_of_intention_cnt\"] += int(row[12])\n",
    "                curr_dict[row[1]][\"type_of_solving_question_cnt\"] += int(row[13])\n",
    "                curr_dict[row[1]][\"type_of_starter_cnt\"] += int(row[14])\n",
    "                curr_dict[row[1]][\"has_tags\"].add(int(row[3]))\n",
    "        else:\n",
    "            curr_dict[row[1]] = {}\n",
    "            if row[2] == 1:\n",
    "                curr_dict[row[1]][\"lecture_bool\"] = 1\n",
    "                curr_dict[row[1]][\"part_1_cnt\"] = int(row[4])\n",
    "                curr_dict[row[1]][\"part_2_cnt\"] = int(row[5])\n",
    "                curr_dict[row[1]][\"part_3_cnt\"] = int(row[6])\n",
    "                curr_dict[row[1]][\"part_4_cnt\"] = int(row[7])\n",
    "                curr_dict[row[1]][\"part_5_cnt\"] = int(row[8])\n",
    "                curr_dict[row[1]][\"part_6_cnt\"] = int(row[9])\n",
    "                curr_dict[row[1]][\"part_7_cnt\"] = int(row[10])\n",
    "                curr_dict[row[1]][\"type_of_concept_cnt\"] = int(row[11])\n",
    "                curr_dict[row[1]][\"type_of_intention_cnt\"] = int(row[12])\n",
    "                curr_dict[row[1]][\"type_of_solving_question_cnt\"] = int(row[13])\n",
    "                curr_dict[row[1]][\"type_of_starter_cnt\"] = int(row[14])\n",
    "                curr_dict[row[1]][\"has_tags\"] = set([int(row[3])])\n",
    "            else:\n",
    "                curr_dict[row[1]][\"lecture_bool\"] = 0\n",
    "                curr_dict[row[1]][\"part_1_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_2_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_3_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_4_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_5_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_6_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"part_7_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"type_of_concept_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"type_of_intention_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"type_of_solving_question_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"type_of_starter_cnt\"] = 0\n",
    "                curr_dict[row[1]][\"has_tags\"] = set()\n",
    "        \n",
    "        ulc_lb[cnt] = curr_dict[row[1]][\"lecture_bool\"]\n",
    "        part1_l[cnt] = curr_dict[row[1]][\"part_1_cnt\"]\n",
    "        part2_l[cnt] = curr_dict[row[1]][\"part_2_cnt\"]\n",
    "        part3_l[cnt] = curr_dict[row[1]][\"part_3_cnt\"]\n",
    "        part4_l[cnt] = curr_dict[row[1]][\"part_4_cnt\"]\n",
    "        part5_l[cnt] = curr_dict[row[1]][\"part_5_cnt\"]\n",
    "        part6_l[cnt] = curr_dict[row[1]][\"part_6_cnt\"]\n",
    "        part7_l[cnt] = curr_dict[row[1]][\"part_7_cnt\"]\n",
    "        type_of_concept_l[cnt] = curr_dict[row[1]][\"type_of_concept_cnt\"]\n",
    "        type_of_intention_l[cnt] = curr_dict[row[1]][\"type_of_intention_cnt\"]\n",
    "        type_of_solving_question_l[cnt] = curr_dict[row[1]][\"type_of_solving_question_cnt\"]\n",
    "        type_of_starter_l[cnt] = curr_dict[row[1]][\"type_of_starter_cnt\"]\n",
    "        \n",
    "        if type(row[15]) == list:\n",
    "            tags_has = 0\n",
    "            for tag in row[15]:\n",
    "                if int(tag) in curr_dict[row[1]][\"has_tags\"]:\n",
    "                    tags_has += 1\n",
    "            has_tags_l[cnt] = tags_has/len(row[15])\n",
    "\n",
    "    has_tags_lb = (has_tags_l > 0).astype(\"int8\")\n",
    "\n",
    "    lectures_feats_df = pd.DataFrame({\"curr_lecture_bool\":ulc_lb, # 这个用户之前是否听过讲座\n",
    "                                      \"part_1_cnt\":part1_l, # 这个用户听过多少次part类型为1的讲座\n",
    "                                      \"part_2_cnt\":part2_l, # 这个用户听过多少次part类型为2的讲座\n",
    "                                      \"part_3_cnt\":part3_l, # 这个用户听过多少次part类型为3的讲座\n",
    "                                      \"part_4_cnt\":part4_l, # 这个用户听过多少次part类型为4的讲座\n",
    "                                      \"part_5_cnt\":part5_l, # 这个用户听过多少次part类型为5的讲座\n",
    "                                      \"part_6_cnt\":part6_l, # 这个用户听过多少次part类型为6的讲座\n",
    "                                      \"part_7_cnt\":part7_l, # 这个用户听过多少次part类型为7的讲座\n",
    "                                      \"type_of_concept_cnt\":type_of_concept_l, # 这个用户听过多少次type类型为concept的讲座\n",
    "                                      \"type_of_intention_cnt\":type_of_intention_l, # 这个用户听过多少次type类型为intention的讲座\n",
    "                                      \"type_of_solving_question_cnt\":type_of_solving_question_l, # 这个用户听过多少次type类型为solving_question的讲座\n",
    "                                      \"type_of_starter_cnt\":type_of_starter_l, # 这个用户听过多少次type类型为starter的讲座\n",
    "                                      \"watched_tags_rate\":has_tags_l, # 这个用户在做的这个question所包含的tags，有多少比例是他以前看过相同tags的讲座\n",
    "                                      \"watched_tags_bool\":has_tags_lb,# 这个用户在做的这个question所包含的tags，是否至少有一个tag是他以前看过相同tag的讲座\n",
    "                                     }).set_index(new_df[\"row_id\"])\n",
    "\n",
    "    df = df.merge(lectures_feats_df,how=\"left\",left_on=\"row_id\",right_index=True)\n",
    "    return df\n",
    "\n",
    "curr_lectures_dict = {}\n",
    "train_df = add_lectures_feats(train_df, curr_lectures_dict)\n",
    "valid_df = add_lectures_feats(valid_df, curr_lectures_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 剔除lectures行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_ques: (96817414, 23) Valid_ques: (2453886, 23)\n"
     ]
    }
   ],
   "source": [
    "# 只保留question行\n",
    "train_df = train_df[train_df['content_type_id'] == 0].reset_index(drop=True)\n",
    "valid_df = valid_df[valid_df['content_type_id'] == 0].reset_index(drop=True)\n",
    "print(\"Train_ques:\", train_df.shape,\"Valid_ques:\", valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成每个content 的 elapsed_time 和 had_explanation的平均值\n",
    "content_elapsed_time_agg=train_df.groupby('content_id')['prior_question_elapsed_time'].agg(['mean']).astype('float32')\n",
    "content_had_explanation_agg=train_df.groupby('content_id')['prior_question_had_explanation'].agg(['mean']).astype('float32')\n",
    "content_elapsed_time_agg.columns = [\"content_elapsed_time\"]\n",
    "content_had_explanation_agg.columns = [\"content_had_explanation\"]\n",
    "\n",
    "train_df = train_df.merge(content_elapsed_time_agg, how=\"left\", left_on=\"content_id\", right_index=True)\n",
    "valid_df = valid_df.merge(content_elapsed_time_agg, how=\"left\", left_on=\"content_id\", right_index=True)\n",
    "train_df = train_df.merge(content_had_explanation_agg, how=\"left\", left_on=\"content_id\", right_index=True)\n",
    "valid_df = valid_df.merge(content_had_explanation_agg, how=\"left\", left_on=\"content_id\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 生成 每个task_container_id 会被他所关联过的所有用户，平均关联几次\n",
    "task_user_df = train_df[['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count','nunique'])\n",
    "task_user_df.columns = ['cnt',\"unq\"]\n",
    "task_user_df[\"avg_task_seen\"] = task_user_df[\"cnt\"]/task_user_df[\"unq\"]\n",
    "task_user_df['avg_task_seen_cumsum'] = task_user_df.avg_task_seen.cumsum()\n",
    "task_user_df = task_user_df[[\"avg_task_seen_cumsum\"]].astype(np.float32)\n",
    "\n",
    "train_df = train_df.merge(task_user_df, how='left', left_on='task_container_id',right_index=True)\n",
    "valid_df = valid_df.merge(task_user_df, how='left', left_on='task_container_id',right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 每个question的正确率\n",
    "content_answers_df = train_df[['content_id','answered_correctly']].groupby('content_id').agg([\"mean\",\"count\"])\n",
    "content_answers_df.columns = ['content_mean_acc','content_cnt']\n",
    "content_answers_df['content_cnt'] = content_answers_df['content_cnt'].astype(\"uint32\")\n",
    "content_answers_df['content_mean_acc'] = content_answers_df['content_mean_acc'].astype(np.float32)\n",
    "\n",
    "train_df = train_df.merge(content_answers_df, how='left', left_on='content_id', right_index=True)\n",
    "valid_df = valid_df.merge(content_answers_df, how='left', left_on='content_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 生成 回答正确的question平均花费的elapsed_time\n",
    "# 生成 回答正确的question平均花费的elapsed_time\n",
    "# 文件在百度网盘\n",
    "with open(\"D:/kaggle/input/riiid-test-answer-prediction/question_elapsed_time_mean.pkl.data\",\"rb\") as f:\n",
    "    question_elapsed_time_df = pickle.load(f)\n",
    "train_df = train_df.merge(question_elapsed_time_df,on = \"content_id\", how = \"left\")\n",
    "valid_df = valid_df.merge(question_elapsed_time_df,on = \"content_id\", how = \"left\")\n",
    "\n",
    "train_df.corr_question_elapsed_time_mean = train_df.corr_question_elapsed_time_mean.fillna(-1).astype(\"float32\")\n",
    "train_df.incorr_question_elapsed_time_mean = train_df.incorr_question_elapsed_time_mean.fillna(-1).astype(\"float32\")\n",
    "valid_df.corr_question_elapsed_time_mean = valid_df.corr_question_elapsed_time_mean.fillna(-1).astype(\"float32\")\n",
    "valid_df.incorr_question_elapsed_time_mean = valid_df.incorr_question_elapsed_time_mean.fillna(-1).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 生成 回答正确的question，用户在回答问题前看了之前题目的详解的概率\n",
    "# 生成 回答错误的question，用户在回答问题前看了之前题目的详解的概率\n",
    "train_df['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "content_explation_agg=train_df[[\"content_id\",\"prior_question_had_explanation\",'answered_correctly']].groupby(\n",
    "    [\"content_id\",\"prior_question_had_explanation\"])['answered_correctly'].agg(['mean'])\n",
    "\n",
    "content_explation_agg = content_explation_agg.unstack()\n",
    "content_explation_agg = content_explation_agg.reset_index()\n",
    "content_explation_agg.columns = ['content_id', 'content_explation_false_mean','content_explation_true_mean']\n",
    "content_explation_agg.content_id = content_explation_agg.content_id.astype('int16')\n",
    "content_explation_agg.content_explation_false_mean = content_explation_agg.content_explation_false_mean.astype('float32')\n",
    "content_explation_agg.content_explation_true_mean = content_explation_agg.content_explation_true_mean.astype('float32')\n",
    "\n",
    "train_df = train_df.merge(content_explation_agg,how=\"left\",left_on=\"content_id\",right_on=\"content_id\")\n",
    "valid_df = valid_df.merge(content_explation_agg,how=\"left\",left_on=\"content_id\",right_on=\"content_id\")\n",
    "train_df[\"content_explation_false_mean\"].fillna(0,inplace=True)\n",
    "train_df[\"content_explation_true_mean\"].fillna(0,inplace=True)\n",
    "valid_df[\"content_explation_false_mean\"].fillna(0,inplace=True)\n",
    "valid_df[\"content_explation_true_mean\"].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 用户做这题时，距离上次（前一次）做题过去了多少时间\n",
    "# 生成 用户做这题时，距离上上次（前两次）做题过去了多少时间\n",
    "# 生成 用户做这题时，距离上上上次（前三次）做题过去了多少时间\n",
    "# section1\n",
    "max_timestamp_u = train_df[['user_id','timestamp']].groupby(['user_id']).agg(['max']).reset_index()\n",
    "max_timestamp_u.columns = ['user_id', 'max_time_stamp']\n",
    "max_timestamp_u.user_id=max_timestamp_u.user_id.astype('int32')\n",
    "\n",
    "train_df['lagtime'] = train_df.groupby('user_id')['timestamp'].shift()\n",
    "max_timestamp_u2 = train_df[['user_id','lagtime']].groupby(['user_id']).agg(['max']).reset_index()\n",
    "max_timestamp_u2.columns = ['user_id', 'max_time_stamp2']\n",
    "max_timestamp_u2.user_id=max_timestamp_u2.user_id.astype('int32')\n",
    "\n",
    "train_df['lagtime']=train_df['timestamp']-train_df['lagtime']\n",
    "lagtime_mean=train_df['lagtime'].mean()\n",
    "train_df['lagtime'].fillna(lagtime_mean, inplace=True)\n",
    "\n",
    "\n",
    "# section2\n",
    "train_df['lagtime2'] = train_df.groupby('user_id')['timestamp'].shift(2)\n",
    "max_timestamp_u3 = train_df[['user_id','lagtime2']].groupby(['user_id']).agg(['max']).reset_index()\n",
    "max_timestamp_u3.columns = ['user_id', 'max_time_stamp3']\n",
    "max_timestamp_u3.user_id=max_timestamp_u3.user_id.astype('int32')\n",
    "train_df['lagtime2']=train_df['timestamp']-train_df['lagtime2']\n",
    "lagtime_mean2=train_df['lagtime2'].mean()\n",
    "train_df['lagtime2'].fillna(0, inplace=True)\n",
    "\n",
    "# section3\n",
    "train_df['lagtime3'] = train_df.groupby('user_id')['timestamp'].shift(3)\n",
    "\n",
    "train_df['lagtime3']=train_df['timestamp']-train_df['lagtime3']\n",
    "lagtime_mean3=train_df['lagtime3'].mean()\n",
    "train_df['lagtime3'].fillna(0, inplace=True)\n",
    "\n",
    "# gen_dict\n",
    "max_timestamp_u_dict=max_timestamp_u.set_index('user_id').to_dict()\n",
    "max_timestamp_u_dict2=max_timestamp_u2.set_index('user_id').to_dict()\n",
    "max_timestamp_u_dict3=max_timestamp_u3.set_index('user_id').to_dict()\n",
    "del max_timestamp_u\n",
    "del max_timestamp_u2\n",
    "del max_timestamp_u3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同上\n",
    "lagtime2 = np.zeros(len(valid_df), dtype=np.float32)\n",
    "lagtime3 = np.zeros(len(valid_df), dtype=np.float32)\n",
    "for i, (user_id,\n",
    "        content_type_id,\n",
    "        timestamp,\n",
    "        content_id,) in enumerate(zip(\n",
    "    valid_df['user_id'].values,\n",
    "    valid_df['content_type_id'].values,\n",
    "    valid_df['timestamp'].values,\n",
    "    valid_df['content_id'].values)):\n",
    "    if content_type_id==0:\n",
    "        if user_id in max_timestamp_u_dict['max_time_stamp'].keys():\n",
    "            if(max_timestamp_u_dict2['max_time_stamp2'][user_id]==0):\n",
    "                lagtime2[i]=0\n",
    "                lagtime3[i]=0\n",
    "            else:\n",
    "                lagtime2[i]=timestamp-max_timestamp_u_dict2['max_time_stamp2'][user_id]\n",
    "                if(max_timestamp_u_dict3['max_time_stamp3'][user_id]==0):\n",
    "                    lagtime3[i]=0\n",
    "                else:\n",
    "                    lagtime3[i]=timestamp-max_timestamp_u_dict3['max_time_stamp3'][user_id]\n",
    "                max_timestamp_u_dict3['max_time_stamp3'][user_id]=max_timestamp_u_dict2['max_time_stamp2'][user_id]\n",
    "            max_timestamp_u_dict2['max_time_stamp2'][user_id]=max_timestamp_u_dict['max_time_stamp'][user_id]\n",
    "            max_timestamp_u_dict['max_time_stamp'][user_id]=timestamp\n",
    "        else:\n",
    "            max_timestamp_u_dict['max_time_stamp'].update({user_id:timestamp})\n",
    "            lagtime2[i]=0\n",
    "            max_timestamp_u_dict2['max_time_stamp2'].update({user_id:0})\n",
    "            lagtime3[i]=0\n",
    "            max_timestamp_u_dict3['max_time_stamp3'].update({user_id:0})\n",
    "            \n",
    "valid_df[\"lagtime2\"]=lagtime2\n",
    "valid_df[\"lagtime3\"]=lagtime3\n",
    "valid_df[\"lagtime2\"].fillna(0, inplace=True)\n",
    "valid_df[\"lagtime3\"].fillna(0, inplace=True)\n",
    "train_df[\"lagtime2\"] = train_df[\"lagtime2\"].astype(\"uint64\")\n",
    "train_df[\"lagtime3\"] = train_df[\"lagtime3\"].astype(\"uint64\")\n",
    "valid_df[\"lagtime2\"] = valid_df[\"lagtime2\"].astype(\"uint64\")\n",
    "valid_df[\"lagtime3\"] = valid_df[\"lagtime3\"].astype(\"uint64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questions.csv\n",
    "merge question.csv文件 生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 将 part 和 bundle_id 直接组合形成一个新的特征\n",
    "ques_df3 = pd.read_csv(\"D:/kaggle/input/riiid-test-answer-prediction/questions.csv\")\n",
    "ques_df3['part_bundle_id']=(ques_df3['part']*100000+ques_df3['bundle_id']).astype('int32')\n",
    "ques_df3 = ques_df3[[\"part_bundle_id\"]]\n",
    "\n",
    "train_df = train_df.merge(ques_df3, how=\"left\", left_on=\"content_id\", right_index=True)\n",
    "valid_df = valid_df.merge(ques_df3, how=\"left\", left_on=\"content_id\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 生成 每个question的这些tags的平均正确率。\n",
    "# 解释一下：\n",
    "# 首先每个question是有一个所有用户答题的正确率（就是关心这个question是难题还是简单题）\n",
    "# 然后每个tags归属于若干的question。那么可以求出每个tags所属的那些问题的平均正确率。\n",
    "# 得到tags的正确率以后，又因为每个question又包含若干的tags，那么可以求出每个question的这些tags的平均正确率。\n",
    "qdf = pd.read_csv(\"D:/kaggle/input/riiid-test-answer-prediction/questions.csv\")[['tags']].astype(str)\n",
    "qdf['tags_l'] = [x.split() for x in qdf.tags.values]\n",
    "tags_set_list = [str(i) for i in list(range(188))]\n",
    "\n",
    "ques_correct_df = train_df.groupby([\"content_id\"]).agg({\"answered_correctly\": [\"count\",\"sum\"]})\n",
    "ques_correct_df.columns = [\"total\", \"right\"]\n",
    "ques_correct_df[['total', 'right']] = ques_correct_df[['total', 'right']].astype(int)\n",
    "qdf = qdf.merge(ques_correct_df, left_index=True, right_index=True, how = \"left\")\n",
    "\n",
    "tags_dict = {}\n",
    "for x in tags_set_list:\n",
    "    tags_dict[x] = [0, 0]\n",
    "    for y in range(len(qdf)):\n",
    "        if x in qdf.tags_l[y]:\n",
    "            tags_dict[x][0] += qdf.right[y]\n",
    "            tags_dict[x][1] += qdf.total[y]\n",
    "            \n",
    "def get_tags_acc(x):\n",
    "    if [\"nan\"] == x:\n",
    "        return 0.65\n",
    "    right = 0; total = 0\n",
    "    for tag in x:\n",
    "        right += tags_dict[tag][0]\n",
    "        total += tags_dict[tag][1]\n",
    "    return right/total\n",
    "\n",
    "qdf = qdf[\"tags_l\"].apply(get_tags_acc).rename(\"tags_acc\").astype(np.float32)\n",
    "\n",
    "train_df = train_df.merge(qdf,how=\"left\",left_on=\"content_id\",right_index=True)\n",
    "valid_df = valid_df.merge(qdf,how=\"left\",left_on=\"content_id\",right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#生成 每个 part-bundle对（groupby(['bundle_id', 'part']），他的平均正确率\n",
    "ques_columns = [\"question_id\", \"part\", \"bundle_id\"]\n",
    "ques_df = pd.read_csv(\"D:/kaggle/input/riiid-test-answer-prediction/questions.csv\")[ques_columns]\n",
    "ques_df[\"part\"] = ques_df[\"part\"].astype(\"int8\")\n",
    "\n",
    "part_bundle_df = train_df.merge(ques_df, how=\"left\",left_on='content_id', right_on='question_id')\n",
    "part_bundle_df = part_bundle_df[['bundle_id','part','answered_correctly']].groupby(['bundle_id', 'part'],as_index=False).agg({\"answered_correctly\":[\"mean\"]}).reset_index(drop=True)\n",
    "part_bundle_df.columns = [\"bundle_id\", \"part\", 'part_bundle_acc']\n",
    "\n",
    "ques_df = ques_df.merge(part_bundle_df,how=\"left\", on=[\"part\",\"bundle_id\"])\n",
    "ques_df = ques_df[[\"question_id\",\"part\",\"part_bundle_acc\"]].set_index(\"question_id\")\n",
    "ques_df[\"part_bundle_acc\"] = ques_df[\"part_bundle_acc\"].astype(np.float32)\n",
    "\n",
    "train_df = train_df.merge(ques_df, how='left', left_on='content_id',right_index=True)\n",
    "valid_df = valid_df.merge(ques_df, how='left', left_on='content_id',right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 用于user-part对（groupby(['user_id', 'part'])）的正确率\n",
    "#注意！这里首次出现了关于用户的正确率。用户的正确率会和做题的数量成正比，所以不能全局的直接求出一个用户平均的正确率，一定要求他当前做题时的过往正确率。\n",
    "\n",
    "# 用于train data\n",
    "curr_up_dict_df = train_df.groupby(['user_id', 'part'])[\"answered_correctly\"].agg(['count', 'sum']).astype('uint16')\n",
    "train_df['lag'] = train_df.groupby(['user_id', 'part'])[\"answered_correctly\"].shift()\n",
    "cum = train_df.groupby(['user_id', 'part'])['lag'].agg(['cumsum', 'cumcount'])\n",
    "train_df[\"curr_user_part_count\"] = cum['cumcount'].astype('uint16')\n",
    "train_df[\"curr_user_part_sum\"] = cum['cumsum']\n",
    "train_df['curr_user_part_acc'] = (train_df[\"curr_user_part_sum\"] / train_df[\"curr_user_part_count\"]).fillna(0.68).astype(\"float32\")\n",
    "train_df[\"curr_user_part_sum\"] = train_df[\"curr_user_part_sum\"].fillna(0).astype('uint16')\n",
    "train_df.drop(columns=['lag'], inplace=True)\n",
    "\n",
    "# 用于valid data和test data\n",
    "part_user_d = curr_up_dict_df.to_dict(\"index\")\n",
    "np_up_cnt = np.zeros((len(valid_df),2), dtype=np.uint16)\n",
    "for idx, (user_id, answered_correctly, part) in enumerate(zip(valid_df['user_id'].values, valid_df['answered_correctly'].values, valid_df['part'].values)):\n",
    "    if (user_id,part) in part_user_d:\n",
    "        np_up_cnt[idx] = [part_user_d[(user_id,part)][\"count\"], part_user_d[(user_id,part)][\"sum\"]]\n",
    "        part_user_d[(user_id,part)][\"count\"] += 1\n",
    "        part_user_d[(user_id,part)][\"sum\"] += answered_correctly\n",
    "    else:\n",
    "        part_user_d[(user_id,part)] = {'count': 1, 'sum': 1} if answered_correctly == 1 else {'count': 1, 'sum': 0}\n",
    "            \n",
    "curr_user_part_df = pd.DataFrame(np_up_cnt,columns=[\"curr_user_part_count\", \"curr_user_part_sum\"])\n",
    "curr_user_part_df[\"curr_user_part_acc\"] = (curr_user_part_df[\"curr_user_part_sum\"] / curr_user_part_df[\"curr_user_part_count\"]).fillna(0.68).astype(np.float32)\n",
    "valid_df = valid_df.merge(curr_user_part_df,how=\"left\", left_index=True, right_index=True)\n",
    "  \n",
    "del cum\n",
    "del curr_user_part_df\n",
    "curr_up_dict_df = pd.DataFrame.from_dict(part_user_d,\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lectures和question混合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cf142174db45749b0bc60e4525cd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=96817414.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# 生成 用户在做某个part类型的题目前，看过多少个part类型的讲座\n",
    "def same_part(df):\n",
    "    same_part_l = np.zeros(len(df), dtype=\"uint8\")\n",
    "    for idx,row in enumerate(tqdm(df.itertuples(), total=df.shape[0])):\n",
    "        part_cnt = eval(f\"row.part_{str(row.part)}_cnt\")\n",
    "        same_part_l[idx] = part_cnt\n",
    "    df[\"same_part_cnt\"] = same_part_l\n",
    "    return df\n",
    "\n",
    "train_df = same_part(train_df)\n",
    "valid_df = same_part(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 生成讲座相关的特征（和时间序列相关）， 具体生成特征解释在函数末尾处\n",
    "# 以下三个函数，第一个用于train_data or valid_data，后两个用于test_data\n",
    "# 为什么要分三个函数？\n",
    "# 因为比赛test_data 以api方式生成，每一批传过来数据，包含当前批次的数据的特征，和上一批的数据的label。\n",
    "# 所以在当你拿到test_data需要生成特征时，你需要用你记录的，这个用户以前答题情况来生成特征（第二个函数的作用）\n",
    "# 但是你需要在下一批test_data传过来的时候才能拿到label，你才能（用第三个函数）更新你的对这个用户的记录。\n",
    "def add_user_feats(df):\n",
    "    '''\n",
    "    本函数用作train_data和valid_data的生成\n",
    "    input: train_data or valid_data\n",
    "    return: train_data or valid_data\n",
    "    '''\n",
    "    ucc = np.zeros(len(df), dtype=np.uint16)\n",
    "    uac = np.zeros(len(df), dtype=np.uint16)\n",
    "    uqcor = np.zeros(len(df), dtype=np.uint8)\n",
    "    uqcnt = np.zeros(len(df), dtype=np.uint8)\n",
    "    utdiff = np.zeros(len(df), dtype=np.uint64)\n",
    "    utdiff_mean = np.zeros(len(df), dtype=np.uint64) \n",
    "    uelapdiff = np.zeros(len(df), dtype=np.float32)  \n",
    "    uq_timediff = np.zeros(len(df), dtype=np.uint64) \n",
    "    global idx\n",
    "    \n",
    "    for cnt,row in enumerate(tqdm(df[['user_id','content_id','answered_correctly',\n",
    "                                      'timestamp','prior_question_elapsed_time',\n",
    "                                     ]].itertuples(index=False),total=df.shape[0])): \n",
    "        if row[0] in curr_u_dict:\n",
    "            # 写入np\n",
    "            ucc[cnt] = curr_u_dict[row[0]][\"ucc\"]\n",
    "            uac[cnt] = curr_u_dict[row[0]][\"uac\"]\n",
    "            utdiff[cnt] = row[3] - curr_u_dict[row[0]][\"uts\"]\n",
    "            utdiff_mean[cnt] = curr_u_dict[row[0]][\"utsdiff\"][1] / curr_u_dict[row[0]][\"utsdiff\"][0]\n",
    "            uelapdiff[cnt] = row[4] - curr_u_dict[row[0]][\"uelapdiff\"]\n",
    "            # 写入字典\n",
    "            curr_u_dict[row[0]][\"uts\"] = row[3]\n",
    "            curr_u_dict[row[0]][\"ucc\"] += row[2]\n",
    "            curr_u_dict[row[0]][\"uac\"] += 1\n",
    "            curr_u_dict[row[0]][\"utsdiff\"][0] += 1 \n",
    "            curr_u_dict[row[0]][\"utsdiff\"][1] += row[3] \n",
    "            curr_u_dict[row[0]][\"uelapdiff\"] = row[4] \n",
    "            if row[1] in curr_u_dict[row[0]]:\n",
    "                tmp_idx = curr_u_dict[row[0]][row[1]]\n",
    "                uq_timediff[cnt] =  row[3] - np_uctdiff_cnt[tmp_idx] \n",
    "                uqcor[cnt] = np_cor_cnt[tmp_idx]\n",
    "                uqcnt[cnt] = np_all_cnt[tmp_idx]\n",
    "                np_uctdiff_cnt[tmp_idx] = row[3] \n",
    "                np_cor_cnt[tmp_idx] += row[2]\n",
    "                np_all_cnt[tmp_idx] += 1\n",
    "            else:\n",
    "                uqcor[cnt] = 0; uqcnt[cnt] = 0;\n",
    "                uq_timediff[cnt] = 0 \n",
    "                curr_u_dict[row[0]][row[1]] = idx\n",
    "                np_uctdiff_cnt[idx] = row[3] \n",
    "                np_cor_cnt[idx] += row[2]\n",
    "                np_all_cnt[idx] += 1\n",
    "                idx += 1\n",
    "        else:\n",
    "            # 写入np\n",
    "            ucc[cnt] = 0; uac[cnt] = 0;\n",
    "            uqcor[cnt] = 0; uqcnt[cnt] = 0;\n",
    "            utdiff[cnt] = 0; utdiff_mean[cnt] = 0; \n",
    "            uelapdiff[cnt] = 0; uq_timediff[cnt] = 0 \n",
    "            # 写入字典\n",
    "            curr_u_dict[row[0]] = {}\n",
    "            curr_u_dict[row[0]][\"ucc\"] = row[2]\n",
    "            curr_u_dict[row[0]][\"uac\"] = 1\n",
    "            curr_u_dict[row[0]][\"uts\"] = row[3]\n",
    "            curr_u_dict[row[0]][\"utsdiff\"] = [1, row[3]] \n",
    "            curr_u_dict[row[0]][\"uelapdiff\"] = row[4] \n",
    "            curr_u_dict[row[0]][row[1]] = idx\n",
    "            np_uctdiff_cnt[idx] = row[3] \n",
    "            np_cor_cnt[idx] += row[2]\n",
    "            np_all_cnt[idx] += 1\n",
    "            idx += 1\n",
    "            \n",
    "    user_feats_df = pd.DataFrame({'curr_user_correct_cnt':ucc, # 用户当前答题正确的次数\n",
    "                                  'curr_user_answer_cnt':uac, # 用户当前答题总次数\n",
    "                                  'curr_uq_correct_cnt':uqcor, # 用户回答某一个问题正确的次数\n",
    "                                  'curr_uq_answer_cnt':uqcnt, # 用户回答某一个问题的总次数\n",
    "                                  'curr_user_time_diff':utdiff, # 用户当前距离他第一次答题，过去的时间\n",
    "                                  'curr_user_time_diff_mean':utdiff_mean,  # 用户每一次答题的平均间隔\n",
    "                                  'curr_user_elapsed_time_diff':uelapdiff, # 用户回答上一组问题的平均时间\n",
    "                                  'curr_uq_time_diff':uq_timediff # 用户答题时，距离上次回答这个相同问题过去多少时间\n",
    "                                 }) \n",
    "    user_feats_df['curr_uq_acc'] = user_feats_df['curr_uq_correct_cnt'] / user_feats_df['curr_uq_answer_cnt']\n",
    "    user_feats_df['curr_uq_acc'].fillna(0.680, inplace=True)\n",
    "    user_feats_df['curr_uq_acc'] = user_feats_df['curr_uq_acc'].astype(np.float32)\n",
    "    user_feats_df['curr_uq_correct_cnt'] = user_feats_df['curr_uq_correct_cnt'].where(user_feats_df['curr_uq_correct_cnt'] <= 4, 4)\n",
    "    user_feats_df['curr_uq_answer_cnt'] = user_feats_df['curr_uq_answer_cnt'].where(user_feats_df['curr_uq_answer_cnt'] <= 4, 4)\n",
    "    user_feats_df['curr_user_acc'] = user_feats_df['curr_user_correct_cnt'] / user_feats_df['curr_user_answer_cnt']\n",
    "    user_feats_df['curr_user_acc'].fillna(0.680, inplace=True)\n",
    "    user_feats_df['curr_user_acc'] = user_feats_df['curr_user_acc'].astype(np.float32)\n",
    "    user_feats_df['curr_user_elapsed_time_diff'].fillna(0, inplace=True) \n",
    "    df = pd.concat([df, user_feats_df], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def add_user_feats_without_update(df):\n",
    "    '''\n",
    "    本函数用作test_data的生成\n",
    "    input: test_data\n",
    "    return: test_data\n",
    "    '''\n",
    "    ucc = np.zeros(len(df), dtype=np.uint16)\n",
    "    uac = np.zeros(len(df), dtype=np.uint16)\n",
    "    uqcor = np.zeros(len(df), dtype=np.uint8)\n",
    "    uqcnt = np.zeros(len(df), dtype=np.uint8)\n",
    "    utdiff = np.zeros(len(df), dtype=np.uint64)\n",
    "    utdiff_mean = np.zeros(len(df), dtype=np.uint64) \n",
    "    uelapdiff = np.zeros(len(df), dtype=np.float32)  \n",
    "    uq_timediff = np.zeros(len(df), dtype=np.uint64) \n",
    "    for cnt,row in enumerate(df[['user_id', 'content_id','timestamp','prior_question_elapsed_time']].itertuples(index=False)): \n",
    "        if row[0] in curr_u_dict:\n",
    "            ucc[cnt] = curr_u_dict[row[0]][\"ucc\"]\n",
    "            uac[cnt] = curr_u_dict[row[0]][\"uac\"]\n",
    "            utdiff[cnt] = row[2] - curr_u_dict[row[0]][\"uts\"]\n",
    "            utdiff_mean[cnt] = curr_u_dict[row[0]][\"utsdiff\"][1] / curr_u_dict[row[0]][\"utsdiff\"][0] \n",
    "            uelapdiff[cnt] = row[3] - curr_u_dict[row[0]][\"uelapdiff\"] \n",
    "            if row[1] in curr_u_dict[row[0]]:\n",
    "                tmp_idx = curr_u_dict[row[0]][row[1]]\n",
    "                uq_timediff[cnt] =  row[2] - np_uctdiff_cnt[tmp_idx] \n",
    "                uqcor[cnt] = np_cor_cnt[tmp_idx]\n",
    "                uqcnt[cnt] = np_all_cnt[tmp_idx]\n",
    "            else:\n",
    "                uqcor[cnt] = 0; uqcnt[cnt] = 0\n",
    "                uq_timediff[cnt] = 0 \n",
    "        else:\n",
    "            ucc[cnt] = 0; uac[cnt] = 0\n",
    "            uqcor[cnt] = 0; uqcnt[cnt] = 0\n",
    "            utdiff[cnt] = 0; utdiff_mean[cnt] = 0; \n",
    "            uelapdiff[cnt] = 0; uq_timediff[cnt] = 0 \n",
    "            \n",
    "    user_feats_df = pd.DataFrame({'curr_user_correct_cnt':ucc, 'curr_user_answer_cnt':uac,\n",
    "                                  'curr_uq_correct_cnt':uqcor, 'curr_uq_answer_cnt':uqcnt,\n",
    "                                  'curr_user_time_diff':utdiff, 'curr_user_time_diff_mean':utdiff_mean, \n",
    "                                  'curr_user_elapsed_time_diff':uelapdiff, 'curr_uq_time_diff':uq_timediff \n",
    "                                 }) \n",
    "    user_feats_df['curr_uq_acc'] = user_feats_df['curr_uq_correct_cnt'] / user_feats_df['curr_uq_answer_cnt']\n",
    "    user_feats_df['curr_uq_acc'].fillna(0.680, inplace=True)\n",
    "    user_feats_df['curr_uq_acc'] = user_feats_df['curr_uq_acc'].astype(np.float32)\n",
    "    user_feats_df['curr_uq_correct_cnt'] = user_feats_df['curr_uq_correct_cnt'].where(user_feats_df['curr_uq_correct_cnt'] <= 4, 4)\n",
    "    user_feats_df['curr_uq_answer_cnt'] = user_feats_df['curr_uq_answer_cnt'].where(user_feats_df['curr_uq_answer_cnt'] <= 4, 4)\n",
    "    user_feats_df['curr_user_acc'] = user_feats_df['curr_user_correct_cnt'] / user_feats_df['curr_user_answer_cnt']\n",
    "    user_feats_df['curr_user_acc'].fillna(0.680, inplace=True)\n",
    "    user_feats_df['curr_user_acc'] = user_feats_df['curr_user_acc'].astype(np.float32)\n",
    "    user_feats_df['curr_user_elapsed_time_diff'].fillna(0, inplace=True) \n",
    "    df = pd.concat([df, user_feats_df], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def update_user_feats(df):\n",
    "    '''\n",
    "    本函数用作test_data的生成\n",
    "    input: test_data\n",
    "    return: test_data\n",
    "    '''\n",
    "    global idx\n",
    "    for row in df[['user_id','content_id','answered_correctly','timestamp', 'content_type_id','prior_question_elapsed_time',]].values: \n",
    "        if row[4] == 0:\n",
    "            if row[0] in curr_u_dict:\n",
    "                curr_u_dict[row[0]][\"ucc\"] += row[2]\n",
    "                curr_u_dict[row[0]][\"uac\"] += 1\n",
    "                curr_u_dict[row[0]][\"uts\"] = row[3]\n",
    "                curr_u_dict[row[0]][\"utsdiff\"][0] += 1 \n",
    "                curr_u_dict[row[0]][\"utsdiff\"][1] += row[3] \n",
    "                curr_u_dict[row[0]][\"uelapdiff\"] = row[5] \n",
    "                if row[1] in curr_u_dict[row[0]]:\n",
    "                    tmp_idx = curr_u_dict[row[0]][row[1]]\n",
    "                    np_uctdiff_cnt[tmp_idx] = row[3] \n",
    "                    np_cor_cnt[tmp_idx] += row[2]\n",
    "                    np_all_cnt[tmp_idx] += 1\n",
    "                else:\n",
    "                    curr_u_dict[row[0]][row[1]] = idx\n",
    "                    np_uctdiff_cnt[idx] = row[3] \n",
    "                    np_cor_cnt[idx] += row[2]\n",
    "                    np_all_cnt[idx] += 1\n",
    "                    idx += 1\n",
    "            else:\n",
    "                curr_u_dict[row[0]] = {}\n",
    "                curr_u_dict[row[0]][\"ucc\"] = row[2]\n",
    "                curr_u_dict[row[0]][\"uac\"] = 1\n",
    "                curr_u_dict[row[0]][\"uts\"] = row[3]\n",
    "                curr_u_dict[row[0]][\"utsdiff\"] = [1, row[3]] \n",
    "                curr_u_dict[row[0]][\"uelapdiff\"] = row[5] \n",
    "                curr_u_dict[row[0]][row[1]] = idx\n",
    "                np_uctdiff_cnt[idx] = row[3] \n",
    "                np_cor_cnt[idx] += row[2]\n",
    "                np_all_cnt[idx] += 1\n",
    "                idx += 1\n",
    "                \n",
    "                \n",
    "idx = 0\n",
    "curr_u_dict = {}\n",
    "np_cor_cnt = np.zeros(90_000_000, dtype=np.uint8)\n",
    "np_all_cnt = np.zeros(90_000_000, dtype=np.uint8)\n",
    "np_uctdiff_cnt = np.zeros(90_000_000, dtype=np.uint64)\n",
    "train_df = add_user_feats(train_df)\n",
    "valid_df = add_user_feats(valid_df)\n",
    "print(f\"idx:{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 一些简单的空值填充和异常值处理，前面特征看得懂话，这里应该没问题。\n",
    "\n",
    "train_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].fillna(0).astype(np.int8)\n",
    "valid_df['prior_question_had_explanation'] = valid_df['prior_question_had_explanation'].fillna(0).astype(np.int8)\n",
    "\n",
    "prior_question_elapsed_time_mean = train_df.prior_question_elapsed_time.dropna().values.mean()\n",
    "train_df['prior_question_elapsed_time'] = train_df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n",
    "valid_df['prior_question_elapsed_time'] = valid_df.prior_question_elapsed_time.fillna(prior_question_elapsed_time_mean)\n",
    "\n",
    "train_df['hmean_acc'] = 2*((train_df['curr_user_acc']*train_df['content_mean_acc']) /(train_df['curr_user_acc']+train_df['content_mean_acc'])).astype(np.float32)\n",
    "valid_df['hmean_acc'] = 2*((valid_df['curr_user_acc']*valid_df['content_mean_acc']) /(valid_df['curr_user_acc']+valid_df['content_mean_acc'])).astype(np.float32)\n",
    "\n",
    "train_df[\"content_mean_acc\"] = train_df.content_mean_acc.mask((train_df[\"content_cnt\"] < 3), 0.65)\n",
    "train_df[\"content_mean_acc\"] = train_df.content_mean_acc.mask((train_df[\"content_mean_acc\"] < 0.2) & (train_df[\"content_cnt\"] < 21), 0.2)\n",
    "train_df[\"content_mean_acc\"] = train_df.content_mean_acc.mask((train_df[\"content_mean_acc\"] > 0.95) & (train_df[\"content_cnt\"] < 21), 0.95)\n",
    "\n",
    "train_df[\"curr_user_acc\"] = train_df.curr_user_acc.mask((train_df[\"curr_user_acc\"] < 0.2) & (train_df[\"curr_user_answer_cnt\"] < 21), 0.2)\n",
    "train_df[\"curr_user_acc\"] = train_df.curr_user_acc.mask((train_df[\"curr_user_acc\"] > 0.95) & (train_df[\"curr_user_answer_cnt\"] < 21), 0.95)\n",
    "print(\"Train:\", train_df.shape,\"Valid:\", valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUFFIX = \"_0818\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存一份粗糙的train_df和valid_df，这个不重要，我用来debug用的，真正训练的数据在下面保存\n",
    "# train_df.to_pickle(f\"D:/kaggle/input/riiid-test-answer-prediction/train_df{SUFFIX}_raw.pkl\")\n",
    "# valid_df.to_pickle(f\"D:/kaggle/input/riiid-test-answer-prediction/valid_df{SUFFIX}_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有要用的特征\n",
    "features = [\n",
    "    \"part_bundle_id\",\n",
    "    \n",
    "    'content_elapsed_time',\n",
    "    'content_had_explanation',\n",
    "    \n",
    "    'lagtime2',\n",
    "    'lagtime3',\n",
    "    \n",
    "    'content_explation_false_mean',\n",
    "    'content_explation_true_mean',\n",
    "    \n",
    "    'curr_user_part_acc', \n",
    "    'curr_user_part_count', \n",
    "    'curr_user_part_sum',  \n",
    "    'curr_uq_time_diff', \n",
    "    'curr_user_time_diff',\n",
    "    'curr_user_time_diff_mean',\n",
    "    'curr_user_elapsed_time_diff',\n",
    "\n",
    "    'avg_task_seen_cumsum',\n",
    "    'content_mean_acc',\n",
    "    'content_cnt',\n",
    "    'corr_question_elapsed_time_mean', \n",
    "    'incorr_question_elapsed_time_mean',\n",
    "    \n",
    "    \"watched_tags_rate\",\n",
    "    \"watched_tags_bool\",\n",
    "    'tags_acc',\n",
    "    'part',\n",
    "    'part_bundle_acc', \n",
    "    \n",
    "    'part_1_cnt', 'part_2_cnt', 'part_3_cnt', 'part_4_cnt', 'part_5_cnt', 'part_6_cnt', 'part_7_cnt', \n",
    "    'type_of_concept_cnt', 'type_of_intention_cnt', 'type_of_solving_question_cnt', 'type_of_starter_cnt', \n",
    "    \"same_part_cnt\",\n",
    "    \n",
    "    'curr_lecture_bool',\n",
    "    'curr_user_correct_cnt', \n",
    "    'curr_user_answer_cnt',\n",
    "    'curr_user_acc',\n",
    "    'hmean_acc',\n",
    "    'curr_uq_correct_cnt',\n",
    "    'curr_uq_answer_cnt',\n",
    "    'curr_uq_acc',\n",
    "    'prior_question_elapsed_time',\n",
    "    'prior_question_had_explanation', \n",
    "]\n",
    "\n",
    "target = 'answered_correctly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[features + [target]]\n",
    "valid_df = valid_df[features + [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(train_df.head(10))\n",
    "display(train_df.info())\n",
    "display(train_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(valid_df.head(10))\n",
    "display(valid_df.info())\n",
    "display(valid_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存数据 train_df valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存前面生成的所有特征，这些特征之后上传到kaggle，直接merge就能用，这样kaggle上不用再生成一遍了。\n",
    "tmp_dict1 = pd.DataFrame.from_dict(curr_lectures_dict, orient='index')\n",
    "tmp_dict1.to_csv(f\"D:/kaggle/input/riiid-test-answer-prediction/curr_lectures_dict{SUFFIX}.csv.data\")\n",
    "\n",
    "pickle.dump(content_answers_df, open(f\"D:/kaggle/input/riiid-test-answer-prediction/content_answers_df{SUFFIX}.pkl.data\",\"wb\"))\n",
    "pickle.dump(ques_df, open(f\"D:/kaggle/input/riiid-test-answer-prediction/ques_df{SUFFIX}.pkl.data\",\"wb\"))\n",
    "pickle.dump(task_user_df, open(f\"D:/kaggle/input/riiid-test-answer-prediction/task_user_df{SUFFIX}.pkl.data\",\"wb\"))\n",
    "pickle.dump(qdf, open(f\"D:/kaggle/input/riiid-test-answer-prediction/qdf{SUFFIX}.pkl.data\",\"wb\"))\n",
    "pickle.dump(content_explation_agg, open(f\"D:/kaggle/input/riiid-test-answer-prediction/content_explation_agg{SUFFIX}.pkl.data\",\"wb\"))\n",
    "pickle.dump(max_timestamp_u_dict, open(f\"D:/kaggle/input/riiid-test-answer-prediction/max_timestamp_u_dict{SUFFIX}.pkl.data\",\"wb\"))\n",
    "pickle.dump(max_timestamp_u_dict2, open(f\"D:/kaggle/input/riiid-test-answer-prediction/max_timestamp_u_dict2{SUFFIX}.pkl.data\",\"wb\"))\n",
    "pickle.dump(max_timestamp_u_dict3, open(f\"D:/kaggle/input/riiid-test-answer-prediction/max_timestamp_u_dict3{SUFFIX}.pkl.data\",\"wb\"))\n",
    "\n",
    "pickle.dump(curr_u_dict, open(f\"D:/kaggle/input/riiid-test-answer-prediction/curr_u_dict{SUFFIX}.pkl.data\",\"wb\"))\n",
    "pickle.dump(np_cor_cnt,open(f\"D:/kaggle/input/riiid-test-answer-prediction/np_cor_cnt{SUFFIX}.pkl.data\",\"wb\"))    \n",
    "pickle.dump(np_all_cnt,open(f\"D:/kaggle/input/riiid-test-answer-prediction/np_all_cnt{SUFFIX}.pkl.data\",\"wb\"))\n",
    "pickle.dump(np_uctdiff_cnt,open(f\"D:/kaggle/input/riiid-test-answer-prediction/np_uctdiff_cnt{SUFFIX}.pkl.data\",\"wb\")) \n",
    "\n",
    "pickle.dump(ques_df3,open(f\"D:/kaggle/input/riiid-test-answer-prediction/ques_df3{SUFFIX}.pkl.data\",\"wb\"))    \n",
    "pickle.dump(content_elapsed_time_agg,open(f\"D:/kaggle/input/riiid-test-answer-prediction/content_elapsed_time_agg{SUFFIX}.pkl.data\",\"wb\"))\n",
    "pickle.dump(content_had_explanation_agg,open(f\"D:/kaggle/input/riiid-test-answer-prediction/content_had_explanation_agg{SUFFIX}.pkl.data\",\"wb\")) \n",
    "\n",
    "pickle.dump(curr_up_dict_df, open(f'D:/kaggle/input/riiid-test-answer-prediction/curr_up_dict_df{SUFFIX}.pkl.data', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存真正的训练数据\n",
    "train_df.to_pickle(f\"D:/kaggle/input/riiid-test-answer-prediction/train_df{SUFFIX}.pkl\")\n",
    "valid_df.to_pickle(f\"D:/kaggle/input/riiid-test-answer-prediction/valid_df{SUFFIX}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "291px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
